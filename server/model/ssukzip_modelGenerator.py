from konlpy.tag import Okt
from tensorflow.keras import models, layers, optimizers, losses, metrics
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt
import numpy as np
import json
import os
import nltk

def read_data(filename):
    with open(filename, 'r', encoding='utf-8') as f:
        data = [line.split('\t') for line in f.read().splitlines()]
        data = data[1:] # txt 파일의 헤더(id document label)는 제외하기
    return data

def tokenize(doc):
    # norm은 정규화, stem은 근어로 표시하기를 나타냄
    return ['/'.join(t) for t in Okt().pos(doc, norm=True, stem=True)]

def term_frequency(doc):
    with open('train_docs.json', encoding="utf-8") as f:
        train_docs = json.load(f)
    
    tokens = [t for d in train_docs for t in d[0]]
    text = nltk.Text(tokens, name='NMSC')
    selected_words = [f[0] for f in text.vocab().most_common(10000)]
    return [doc.count(word) for word in selected_words]

def predict_pos_neg(model, review):
    token = tokenize(review)
    tf = term_frequency(token)
    data = np.expand_dims(np.asarray(tf).astype('float32'), axis=0)
    score = float(model.predict(data))
    # print(f"token : {token} / tf : {tf} / data : {data} / pred_score : {score}")
    if(score > 0.5):
        print("[{}]는 {:.2f}% 확률로 긍정 리뷰이지 않을까 추측해봅니다.^^\n".format(review, score * 100))
    else:
        print("[{}]는 {:.2f}% 확률로 부정 리뷰이지 않을까 추측해봅니다.^^;\n".format(review, (1 - score) * 100))
    return score

if __name__=="__main__":
    train_data = read_data('ratings_train.txt')
    test_data = read_data('ratings_test.txt')

    okt = Okt()

    if os.path.isfile('train_docs.json'):
        with open('train_docs.json', encoding="utf-8") as f:
            train_docs = json.load(f)
        with open('test_docs.json', encoding="utf-8") as f:
            test_docs = json.load(f)
    else:
        train_docs = [(tokenize(row[1]), row[2]) for row in train_data]
        test_docs = [(tokenize(row[1]), row[2]) for row in test_data]
        # JSON 파일로 저장
        with open('train_docs.json', 'w', encoding="utf-8") as make_file:
            json.dump(train_docs, make_file, ensure_ascii=False, indent="\t")
        with open('test_docs.json', 'w', encoding="utf-8") as make_file:
            json.dump(test_docs, make_file, ensure_ascii=False, indent="\t")

    tokens = [t for d in train_docs for t in d[0]]
    text = nltk.Text(tokens, name='NMSC')

    # 시간이 꽤 걸립니다! 시간을 절약하고 싶으면 most_common의 매개변수를 줄여보세요.
    selected_words = [f[0] for f in text.vocab().most_common(10000)]
    train_x = [term_frequency(d) for d, _ in train_docs]
    test_x = [term_frequency(d) for d, _ in test_docs]
    train_y = [c for _, c in train_docs]
    test_y = [c for _, c in test_docs]
    
    x_train = np.asarray(train_x).astype('float32')
    x_test = np.asarray(test_x).astype('float32')

    y_train = np.asarray(train_y).astype('float32')
    y_test = np.asarray(test_y).astype('float32')

    model = models.Sequential()
    model.add(layers.Dense(64, activation='relu', input_shape=(10000,)))
    model.add(layers.Dense(64, activation='relu'))
    model.add(layers.Dense(1, activation='sigmoid'))

    model.compile(optimizer=optimizers.RMSprop(lr=0.002),
                  loss=losses.binary_crossentropy,
                  metrics=[metrics.accuracy])

    history = model.fit(x_train, y_train, epochs=10, batch_size=512)
    print(history.history)
    
    plt.plot(history.history['binary_accuracy'])
    plt.plot(history.history['loss'])
    plt.title("Model Accuracy")
    plt.xlabel('Epoch')
    plt.ylabel('Value')
    plt.legend(['binary_accuracy', 'loss'], loc='upper left')
    plt.show()

    results = model.evaluate(x_test, y_test)
    print(f"model : {model.summary()}")
    model.save("ssukzip_Model.h5")